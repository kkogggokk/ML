{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====== 03.31 수 ========= pdf참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "부스팅(Boosting)이란 단순하고 약한 학습기(Weak Learner)들를 결합해서 보다 정확하고 강력한 학습기(Strong Learner)를 만드는 방식.  \n",
    "정확도가 낮은 하나의 모델을 만들어 학습 시킨뒤, 그 모델의 예측 오류는 두 번째 모델이 보완한다. 이 두 모델을 합치면 처음보다는 정확한 모델이 만들어 진다. 합쳐진 모델의 예측 오류는 다음 모델에서 보완하여 계속 더하는 과정을 반복한다.\n",
    "\n",
    "- 약한 학습기들은 앞 학습기가 만든 오류를 줄이는 방향으로 학습한다.\n",
    "- gradient boosting\n",
    "    - 처음 모델은 y를 예측 두번째 부터는 앞 모델이 만든 오류를 예측 그것을 앞 모델에 업데이트하면 오류를 줄일 수 있다.\n",
    "    - 그 오류를 update할 때 뺄까 더할까를 gradient descent 방법을 쓴다. 미분해서 나오는 값의 음수를 취해서 적용. \n",
    "    - 학습률을 작게하면 update가 조금씩 크면 많이 하게 된다. 그래서 크게하면 학습데이터에 너무 맞아 과대적합 될 수 있다.\n",
    "\n",
    "## GradientBoosting\n",
    "- 개별 모델로 Decision Tree 를 사용한다. \n",
    "- depth가 깊지 않은 트리를 많이 연결해서 이전 트리의 오차를 보정해 나가는 방식으로 실행한다.\n",
    "- 오차를 보정할 때 경사하강법(Gradient descent)을 사용한다.\n",
    "- 얕은 트리를 많이 연결하여 각각의 트리가 데이터의 일부에 대해 예측을 잘 수행하도록 하고 그런 트리들이 모여 전체 성능을 높이는 것이 기본 아이디어.\n",
    "- 분류와 회귀 둘다 지원하는 모델 (GradientBoostingClassification, GrandientBoostingRegressor)\n",
    "- 훈련시간이 많이 걸리고, 트리기반 모델의 특성상 희소한 고차원 데이터에서는 성능이 않좋은 단점이 있다.\n",
    "    - 희소한 고차원 --> 희소한 : 값들에 0이 많은 / 고차원 : 컬럼이 많은 \n",
    "\n",
    "### 주요 파라미터\n",
    "- Decision Tree 의 가지치기 관련 매개변수\n",
    "    - 각각의 tree가 복잡한 모델이 되지 않도록 한다. \n",
    "- learning rate\n",
    "    - 이전 tree의 오차를 얼마나 강하게 보정할 것인지 제어하는 값. \n",
    "    - 값이 크면 보정을 강하게 하여 복잡한 모델을 만든다. 학습데이터의 정확도는 올라가지만 과대적합이 날 수있다. \n",
    "    - 값을 작게 잡으면 보정을 약하게 하여 모델의 복잡도를 줄인다. 과대적합을 줄일 수 있지만 성능 자체가 낮아질 수있다.\n",
    "    - 기본값 : 0.1\n",
    "- n_estimators\n",
    "    - tree의 개수 지정. 많을 수록 복잡한 모델이 된다.\n",
    "- n_iter_no_change, validation_fraction\n",
    "    - validation_fraction에 지정한 비율만큼 n_iter_no_change에 지정한 반복 횟수동안 검증점수가 좋아 지지 않으면 훈련을 조기 종료한다.\n",
    "\n",
    "- 보통 max_depth를 낮춰 개별 트리의 복잡도를 낮춘다. (5가 넘지 않게) 그리고 n_estimators를 가용시간, 메모리 한도에 맞춘뒤 적절한 learning_rate을 찾는다.\n",
    "\n",
    "$ 필기 $  \n",
    "- 대용량(행, 컬럼 양이 많은) 일때 성능이 좋다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.8/site-packages (1.3.3)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from xgboost) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data \n",
    "data = load_breast_cancer()\n",
    "X, y = data['data'], data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "y_train.shape # 이정도 양은 그래디언트부스팅을 사용하기에 적합한 수준의 양은 아니다. 양이 많아야 좋음 \n",
    "# 트리모델이기때문에 라벨인코딩 등 처리하면 된다. \n",
    "\n",
    "# model \n",
    "gb = GradientBoostingClassifier(random_state=1)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "pred_train = gb.predict(X_train)\n",
    "pred_test = gb.predict(X_test)\n",
    "\n",
    "accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worst radius               0.383871\n",
       "worst concave points       0.285990\n",
       "worst perimeter            0.130654\n",
       "mean concave points        0.046544\n",
       "worst area                 0.042472\n",
       "worst texture              0.041187\n",
       "worst concavity            0.012790\n",
       "area error                 0.010906\n",
       "mean texture               0.009120\n",
       "mean concavity             0.007964\n",
       "radius error               0.004788\n",
       "concavity error            0.003404\n",
       "worst fractal dimension    0.002838\n",
       "worst symmetry             0.002546\n",
       "mean area                  0.002540\n",
       "fractal dimension error    0.002532\n",
       "mean compactness           0.001572\n",
       "compactness error          0.001384\n",
       "mean perimeter             0.001381\n",
       "smoothness error           0.001340\n",
       "symmetry error             0.001267\n",
       "perimeter error            0.000813\n",
       "mean radius                0.000656\n",
       "mean fractal dimension     0.000625\n",
       "texture error              0.000575\n",
       "worst compactness          0.000097\n",
       "worst smoothness           0.000066\n",
       "mean symmetry              0.000039\n",
       "mean smoothness            0.000032\n",
       "concave points error       0.000008\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트리모델이기 때문에 피처의 중요도 확인 가능 \n",
    "import pandas as pd \n",
    "fi = gb.feature_importances_\n",
    "\n",
    "fi_s = pd.Series(fi, index = data['feature_names'])\n",
    "fi_s.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV 이용해 최적의 하이퍼파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param = {\n",
    "    'n_estimators' : [100, 200, 300, 400, 500], # tree 갯수 (defaul : 100)\n",
    "    'learning_rate' : [0.001, 0.005, 0.01, 0.1], # 학습률 \n",
    "    'max_depth' : range(1,5),\n",
    "    'subsample' : [0.5, 0.7, 1], # 학습시킬 샘플의 비율\n",
    "}\n",
    "gb = GradientBoostingClassifier(random_state=1)\n",
    "gs = GridSearchCV(gb,\n",
    "                  param_grid = param,\n",
    "                  cv = 3, \n",
    "                  scoring='accuracy',\n",
    "                  n_jobs = -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingClassifier(random_state=1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.001, 0.005, 0.01, 0.1],\n",
       "                         'max_depth': range(1, 5),\n",
       "                         'n_estimators': [100, 200, 300, 400, 500],\n",
       "                         'subsample': [0.5, 0.7, 1]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.927302</td>\n",
       "      <td>0.006951</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 2, 'n_esti...</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.992958</td>\n",
       "      <td>0.974178</td>\n",
       "      <td>0.014470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.425580</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 2, 'n_esti...</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.978873</td>\n",
       "      <td>0.974178</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.885066</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 4, 'n_esti...</td>\n",
       "      <td>0.964789</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.974178</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1.223143</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 2, 'n_esti...</td>\n",
       "      <td>0.964789</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.974178</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.849993</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 2, 'n_esti...</td>\n",
       "      <td>0.964789</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.978873</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "203       0.927302      0.006951         0.001932        0.000010   \n",
       "198       0.425580      0.003506         0.001655        0.000112   \n",
       "229       0.885066      0.002948         0.002238        0.000202   \n",
       "206       1.223143      0.003071         0.002308        0.000106   \n",
       "204       0.849993      0.001701         0.002248        0.000077   \n",
       "\n",
       "    param_learning_rate param_max_depth param_n_estimators param_subsample  \\\n",
       "203                 0.1               2                300               1   \n",
       "198                 0.1               2                200             0.5   \n",
       "229                 0.1               4                200             0.7   \n",
       "206                 0.1               2                400               1   \n",
       "204                 0.1               2                400             0.5   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "203  {'learning_rate': 0.1, 'max_depth': 2, 'n_esti...           0.957746   \n",
       "198  {'learning_rate': 0.1, 'max_depth': 2, 'n_esti...           0.971831   \n",
       "229  {'learning_rate': 0.1, 'max_depth': 4, 'n_esti...           0.964789   \n",
       "206  {'learning_rate': 0.1, 'max_depth': 2, 'n_esti...           0.964789   \n",
       "204  {'learning_rate': 0.1, 'max_depth': 2, 'n_esti...           0.964789   \n",
       "\n",
       "     split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "203           0.971831           0.992958         0.974178        0.014470   \n",
       "198           0.971831           0.978873         0.974178        0.003320   \n",
       "229           0.971831           0.985915         0.974178        0.008783   \n",
       "206           0.971831           0.985915         0.974178        0.008783   \n",
       "204           0.971831           0.978873         0.971831        0.005750   \n",
       "\n",
       "     rank_test_score  \n",
       "203                1  \n",
       "198                1  \n",
       "229                1  \n",
       "206                1  \n",
       "204                5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(gs.cv_results_)\n",
    "result_df.sort_values('rank_test_score').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict \n",
    "pred_test = gs.predict(X_test)\n",
    "accuracy_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00158211, 0.01853285, 0.00143937, 0.01113539, 0.00046681,\n",
       "       0.00342275, 0.0032202 , 0.07233424, 0.00441985, 0.00195512,\n",
       "       0.00912057, 0.01049067, 0.00278726, 0.04558971, 0.00395389,\n",
       "       0.00230444, 0.0079779 , 0.00140281, 0.00380318, 0.00751886,\n",
       "       0.18147091, 0.02102962, 0.22247248, 0.06252418, 0.00042761,\n",
       "       0.00888623, 0.01203544, 0.24964542, 0.02643172, 0.00161843])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_importances_ 확인하기. 그리드서치는 ?가 없어 ? \n",
    "model = gs.best_estimator_\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그리디언트 부스팅의 단점을 보완한 모델 \n",
    "- 어떤 단점? 어떤 개선점? 시간이 느리다. 학습시간이 좀 걸린다. / 과적합을 제어할수있는 규제를 제공 \n",
    "- 그래디언트 부스팅보다 빠른거지 XG부스트또한 학습속도가 걸리긴 하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost(Extra Gradient Boost)\n",
    "- https://xgboost.readthedocs.io/\n",
    "- Gradient Boost 알고리즘을 기반으로 개선해서 나온 모델.\n",
    "- 캐글 경진대회에서 상위에 입상한 데이터 과학자들이 사용한 것을 알려저 유명해짐.\n",
    "- Gradient Boost의 단점인 느린수행시간을 해결하고 과적합을 제어할 수 있는 규제를 제공하여 성능을 높임.\n",
    "- 두가지 개발 방법\n",
    "    - [Scikit-learn 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)\n",
    "    - [파이썬 래퍼 XGBoost 모듈 사용](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training)\n",
    "- 설치   \n",
    "``\n",
    "pip install xgboost\n",
    "conda install -y -c anaconda py-xgboost\n",
    "``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: \\ \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "                                                                               failed\n",
      "\n",
      "UnsatisfiableError: The following specifications were found\n",
      "to be incompatible with the existing python installation in your environment:\n",
      "\n",
      "Specifications:\n",
      "\n",
      "  - py-xgboost -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0']\n",
      "\n",
      "Your python: python=3.8\n",
      "\n",
      "If python is on the left-most side of the chain, that's the version you've asked for.\n",
      "When python appears to the right, that indicates that the thing on the left is somehow\n",
      "not available for the python version you are constrained to. Note that conda will not\n",
      "change your python version to a different minor version unless you explicitly specify\n",
      "that.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn 래퍼 XGBoost\n",
    "- XGBoost를 Scikit-learn프레임워크와 연동할 수 있도록 개발됨.\n",
    "- Scikit-learn의 Estimator들과 동일한 패턴으로 코드를 작성할 수 있다.\n",
    "- GridSearchCV나 Pipeline 등 Scikit-learn이 제공하는 다양한 유틸리티들을 사용할 수 있다.\n",
    "- XGBClassifier: 분류\n",
    "- XGBRegressor : 회귀 \n",
    "\n",
    "### 주요 매개변수\n",
    "- learning_rate : 학습률, 보통 0.01 ~ 0.2 사이의 값 사용\n",
    "- n_estimators : week tree 개수\n",
    "- max_depth: 트리의 depth 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-352ece652890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m xgb = XGBClassifier(n_estimators = 200,\n\u001b[1;32m      4\u001b[0m                     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mlibname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         raise XGBoostError(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;34m'Likely causes:\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 200,\n",
    "                    learning_rate = 0.5,\n",
    "                    max_depth = 2,\n",
    "                    random_state =1\n",
    ")\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = xgb.predict(X_train)\n",
    "pred_test = xgb.predict(X_test)\n",
    "\n",
    "accuracy_score(y_train, pred_train), accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dc5c80f81494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "fi = xgb.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
