{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing) 자연어 처리란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어\n",
    "- 사람이 사용하는 고유한 언어\n",
    "- 인공언어의 반대 의미\n",
    "    - 인공언어: 특정 목적을 위해 인위적으로 만든 언어\n",
    "    - ex) 프로그래밍 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리\n",
    "- 사람이 사용하는 자연어를 컴퓨터가 사용할 수 있도록 처리하는 과정.\n",
    "- 자연어 처리 응용분야\n",
    "    - 번역 시스템\n",
    "    - 문서요약\n",
    "    - 감성분석\n",
    "    - 대화형 시스템(챗봇)\n",
    "    - 정보 검색 시스템\n",
    "    - 텍스트 마이닝\n",
    "    - 음성인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분석 수행 프로세스\n",
    "\n",
    "1. 텍스트 전처리 \n",
    "    - 클렌징(cleansing)\n",
    "        - 특수문자, 기호 필요없는 문자 제거\n",
    "        - 대소문자 변경\n",
    "    - stop word(분석에 필요 없는 토큰) 제거\n",
    "    - 텍스트 토큰화\n",
    "        - 분석의 최소단위로 나누는 작업\n",
    "        - 보통 단어단위나 글자단위로 나눈다.\n",
    "    - 어근 추출(Stemming/Lemmatization)을 통한 텍스트 정규화 작업 --> 알파벳만 지원 , 한글은 지원안함. 한글지원은 ??알파이? 이용한다.\n",
    "2. Feature vectorization   \n",
    "    - 문자열 비정형 데이터인 텍스트를 숫자타입의 정형데이터로 만드는 작업\n",
    "    - BOW와 Word2Vec\n",
    "3. 머신러닝 모델 수립, 학습, 예측, 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- Natural Language ToolKit\n",
    "- https://www.nltk.org/\n",
    "- 자연어 처리를 위한 파이썬 패키지\n",
    "\n",
    "## NLTK 설치\n",
    "- nltk 패키지 설치\n",
    "pip 설치\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "conda 설치\n",
    "```\n",
    "conda install -y nltk\n",
    "```\n",
    "\n",
    "- NLTK 추가 패키지 설치\n",
    "```python\n",
    "import nltk\n",
    "nltk.download() # 설치 GUI 프로그램 실행\n",
    "nltk.download('패키지명')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK 주요기능\n",
    "- ### 말뭉치(corpus) 제공\n",
    "    - **말뭉치**: 언어 연구를 위해 텍스트를 컴퓨터가 읽을 수 있는 형태로 모아 놓은 언어 자료를 말한다.\n",
    "    - 예제용 말뭉치 데이터를 제공한다.\n",
    "- ### 텍스트 정규화를 위한 기능 제공\n",
    "    - 토큰 생성\n",
    "    - 여러 언어의 Stop word(불용어) 제공\n",
    "    - 형태소 분석\n",
    "        - 형태소 --> 보통 어간 \n",
    "            - 의미가 있는 가장 작은 말의 단위\n",
    "        - 형태소 분석\n",
    "            - 말뭉치에서 의미있는(분석시 필요한) 형태소들만 추출하는 것           \n",
    "        - 어간추출(Stemming)\n",
    "        - 원형복원(Lemmatization)\n",
    "        - 품사부착(POS tagging - Part Of Speech)\n",
    "- ### 분석기능을 제공해 주는 클래스 제공\n",
    "    - Text\n",
    "    - FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK 텍스트 정규화 기본 문법\n",
    "\n",
    "## 텍스트 토큰화\n",
    "- 분석을 위해 문서(document)를 작은 단위로 나누는 작업.\n",
    "- **토큰(Token)**\n",
    "    - 나뉜 문자열의 단위를 말한다. \n",
    "    - 정하기에 따라 문장, 단어일 수도 있고 문자일 수도 있다. \n",
    "- **Tokenizer**\n",
    "    - 문장을 token으로 나눠주는 함수\n",
    "    - NLTK 주요 tokenizer\n",
    "        - **sent_tokenize()** : 문장단위로 나눠준다.\n",
    "        - **word_tokenize()** : 단어단위로 나눠준다.\n",
    "        - **regexp_tokenize()** : 토큰의 단위를 정규표현식으로 지정\n",
    "        - 반환타입 : 토큰하나 하나를 원소로 하는 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''\n",
    "Beautiful is better than ugly.\n",
    "Explicit is better than implicit.\n",
    "Simple is better than complex.\n",
    "Complex is better than complicated.\n",
    "Flat is better than nested.\n",
    "Sparse is better than dense.\n",
    "Readability counts.\n",
    "Special cases aren't special enough to break the rules.\n",
    "Although practicality beats purity.\n",
    "Errors should never pass silently.\n",
    "Unless explicitly silenced.\n",
    "In the face of ambiguity, refuse the temptation to guess.\n",
    "There should be one-- and preferably only one --obvious way to do it.\n",
    "Although that way may not be obvious at first unless you're Dutch.\n",
    "Now is better than never.\n",
    "Although never is often better than *right* now.\n",
    "If the implementation is hard to explain, it's a bad idea.\n",
    "If the implementation is easy to explain, it may be a good idea.\n",
    "Namespaces are one honking great idea -- let's do more of those!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/master90/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nBeautiful is better than ugly.',\n",
       " 'Explicit is better than implicit.',\n",
       " 'Simple is better than complex.',\n",
       " 'Complex is better than complicated.',\n",
       " 'Flat is better than nested.',\n",
       " 'Sparse is better than dense.',\n",
       " 'Readability counts.',\n",
       " \"Special cases aren't special enough to break the rules.\",\n",
       " 'Although practicality beats purity.',\n",
       " 'Errors should never pass silently.',\n",
       " 'Unless explicitly silenced.',\n",
       " 'In the face of ambiguity, refuse the temptation to guess.',\n",
       " 'There should be one-- and preferably only one --obvious way to do it.',\n",
       " \"Although that way may not be obvious at first unless you're Dutch.\",\n",
       " 'Now is better than never.',\n",
       " 'Although never is often better than *right* now.',\n",
       " \"If the implementation is hard to explain, it's a bad idea.\",\n",
       " 'If the implementation is easy to explain, it may be a good idea.',\n",
       " \"Namespaces are one honking great idea -- let's do more of those!\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장단위로 토큰화 하기 \n",
    "sent_list = nltk.sent_tokenize(txt) # 매개변수에 토큰화할 대상 문서(문자열)를 넣어준다. \n",
    "print(len(sent_list)), type(sent_list)\n",
    "sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Beautiful',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'ugly',\n",
       " '.',\n",
       " 'Explicit',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'implicit',\n",
       " '.',\n",
       " 'Simple',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complex',\n",
       " '.',\n",
       " 'Complex',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complicated',\n",
       " '.',\n",
       " 'Flat',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'nested',\n",
       " '.',\n",
       " 'Sparse',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'dense',\n",
       " '.',\n",
       " 'Readability',\n",
       " 'counts',\n",
       " '.',\n",
       " 'Special',\n",
       " 'cases',\n",
       " 'are',\n",
       " \"n't\",\n",
       " 'special',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'rules',\n",
       " '.',\n",
       " 'Although',\n",
       " 'practicality',\n",
       " 'beats',\n",
       " 'purity',\n",
       " '.',\n",
       " 'Errors',\n",
       " 'should',\n",
       " 'never',\n",
       " 'pass',\n",
       " 'silently',\n",
       " '.',\n",
       " 'Unless',\n",
       " 'explicitly',\n",
       " 'silenced',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'ambiguity',\n",
       " ',',\n",
       " 'refuse',\n",
       " 'the',\n",
       " 'temptation',\n",
       " 'to',\n",
       " 'guess',\n",
       " '.',\n",
       " 'There',\n",
       " 'should',\n",
       " 'be',\n",
       " 'one',\n",
       " '--',\n",
       " 'and',\n",
       " 'preferably',\n",
       " 'only',\n",
       " 'one',\n",
       " '--',\n",
       " 'obvious',\n",
       " 'way',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " '.',\n",
       " 'Although',\n",
       " 'that',\n",
       " 'way',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'obvious',\n",
       " 'at',\n",
       " 'first',\n",
       " 'unless',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'Dutch',\n",
       " '.',\n",
       " 'Now',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'never',\n",
       " '.',\n",
       " 'Although',\n",
       " 'never',\n",
       " 'is',\n",
       " 'often',\n",
       " 'better',\n",
       " 'than',\n",
       " '*',\n",
       " 'right',\n",
       " '*',\n",
       " 'now',\n",
       " '.',\n",
       " 'If',\n",
       " 'the',\n",
       " 'implementation',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'explain',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'bad',\n",
       " 'idea',\n",
       " '.',\n",
       " 'If',\n",
       " 'the',\n",
       " 'implementation',\n",
       " 'is',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'explain',\n",
       " ',',\n",
       " 'it',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'idea',\n",
       " '.',\n",
       " 'Namespaces',\n",
       " 'are',\n",
       " 'one',\n",
       " 'honking',\n",
       " 'great',\n",
       " 'idea',\n",
       " '--',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'do',\n",
       " 'more',\n",
       " 'of',\n",
       " 'those',\n",
       " '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어단위 토큰화(공백기준)\n",
    "word_list = nltk.word_tokenize(txt)\n",
    "print(type(word_list), len(word_list))\n",
    "word_list # 부호 기호들도 들어와있따. \n",
    "# 전처리 한 후에 사용하면 좋음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Beautiful',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'ugly',\n",
       " 'Explicit',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'implicit',\n",
       " 'Simple',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complex',\n",
       " 'Complex',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'complicated',\n",
       " 'Flat',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'nested',\n",
       " 'Sparse',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'dense',\n",
       " 'Readability',\n",
       " 'counts',\n",
       " 'Special',\n",
       " 'cases',\n",
       " 'aren',\n",
       " 't',\n",
       " 'special',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'rules',\n",
       " 'Although',\n",
       " 'practicality',\n",
       " 'beats',\n",
       " 'purity',\n",
       " 'Errors',\n",
       " 'should',\n",
       " 'never',\n",
       " 'pass',\n",
       " 'silently',\n",
       " 'Unless',\n",
       " 'explicitly',\n",
       " 'silenced',\n",
       " 'In',\n",
       " 'the',\n",
       " 'face',\n",
       " 'of',\n",
       " 'ambiguity',\n",
       " 'refuse',\n",
       " 'the',\n",
       " 'temptation',\n",
       " 'to',\n",
       " 'guess',\n",
       " 'There',\n",
       " 'should',\n",
       " 'be',\n",
       " 'one',\n",
       " 'and',\n",
       " 'preferably',\n",
       " 'only',\n",
       " 'one',\n",
       " 'obvious',\n",
       " 'way',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " 'Although',\n",
       " 'that',\n",
       " 'way',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'obvious',\n",
       " 'at',\n",
       " 'first',\n",
       " 'unless',\n",
       " 'you',\n",
       " 're',\n",
       " 'Dutch',\n",
       " 'Now',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " 'never',\n",
       " 'Although',\n",
       " 'never',\n",
       " 'is',\n",
       " 'often',\n",
       " 'better',\n",
       " 'than',\n",
       " 'right',\n",
       " 'now',\n",
       " 'If',\n",
       " 'the',\n",
       " 'implementation',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'it',\n",
       " 's',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'idea',\n",
       " 'If',\n",
       " 'the',\n",
       " 'implementation',\n",
       " 'is',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'it',\n",
       " 'may',\n",
       " 'be',\n",
       " 'a',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'Namespaces',\n",
       " 'are',\n",
       " 'one',\n",
       " 'honking',\n",
       " 'great',\n",
       " 'idea',\n",
       " 'let',\n",
       " 's',\n",
       " 'do',\n",
       " 'more',\n",
       " 'of',\n",
       " 'those']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰의 형식(패턴)을 정규표현식으로 지정 \n",
    "reg_list = nltk.regexp_tokenize(txt, '\\w+') # 토큰의 패턴을 지정 \n",
    "print(len(reg_list))\n",
    "reg_list\n",
    "# 전처리할때 작업함, 단 패턴을 잘 줘야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \\w : 0-9a-zA-Z가-힣_ \n",
    "- + :1 글자이상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword (불용어)\n",
    "- 문장내에서는 많이 사용되지만 문장의 전체 맥락과는 상관없는 단어들.\n",
    "    - ex) 조사, 접미사, 접속사, 대명사 등\n",
    "- Stopword 단어들을 List로 묶어서 관리한다.\n",
    "    - nltk가 언어별로 Stop word 제공.\n",
    "    - 실제 분석대상에 맞는 Stop word 를 만들어서 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/master90/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 <class 'list'>\n",
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n"
     ]
    }
   ],
   "source": [
    "# 언어별 불용어 사전 로딩해보자 \n",
    "sw_arabic = stopwords.words('arabic')\n",
    "print(len(sw_arabic), type(sw_arabic))\n",
    "print(sw_arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 <class 'list'>\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "print(len(sw), type(sw)) \n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize_text()  함수 구현\n",
    "- 문장별 토큰화 시키는 함수 구현\n",
    "- 쉼표,마침표등 구두점(punctuation)은 공백처리한다.\n",
    "- stopword를 제외한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_text 함수 생성 \n",
    "def tokenize_text(doc):\n",
    "    '''\n",
    "    하나의 문서를 받아서 토큰화 해서 반환하는 함수이다.\n",
    "    문장별로 단어리스트를 2차원 배열 형태로 반환한다\n",
    "    구두점이나 특수문자, 숫자, 불용어(stopword)들은 모두 제거한다. \n",
    "    모두 소문자로 구성한다. \n",
    "    \n",
    "    [매개변수]\n",
    "        - doc : 변환대상 문서를 매개변수로 받는다. \n",
    "    [반환값]\n",
    "        - list : 2차원 형태의 리스트로 반환한다. (문장, 단어)\n",
    "    '''\n",
    "    \n",
    "    # 1. 받는 문장을 소문자로 변환한다. \n",
    "    text = doc.lower()\n",
    "    \n",
    "    # 2. 문장단위로 토큰화 진행 \n",
    "    sent_tokens = nltk.sent_tokenize(text) # 마침표, 느낌표 (문장의끝을 알려주는 기호를 기준으로) 문장이 나눠지기 때문에 진행 후 특수문자 제거한다. \n",
    "    \n",
    "    # 3. 불용어stopword 리스트 생성\n",
    "    sw = stopwords.words('english')\n",
    "    sw.extend(['although','unless']) # 불용어 추가 \n",
    "    \n",
    "    result_list = [] # 최종결과 담을 리스트\n",
    "    \n",
    "    # 4. 문장별로 단어단위 토큰화 \n",
    "    for sent in sent_tokens : \n",
    "            # 문장 단위 토큰화 작업, 불용어 제거, 특수문자/숫자 제거 \n",
    "            tmp_words = nltk.regexp_tokenize(sent, '[a-zA-Z가-힣]+')\n",
    "            word_list = [w for w in tmp_words if w not in sw]\n",
    "            result_list.append(word_list)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T22:13:03.097464Z",
     "start_time": "2020-09-07T22:13:03.035595Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beautiful', 'better', 'ugly'],\n",
      " ['explicit', 'better', 'implicit'],\n",
      " ['simple', 'better', 'complex'],\n",
      " ['complex', 'better', 'complicated'],\n",
      " ['flat', 'better', 'nested'],\n",
      " ['sparse', 'better', 'dense'],\n",
      " ['readability', 'counts'],\n",
      " ['special', 'cases', 'special', 'enough', 'break', 'rules'],\n",
      " ['practicality', 'beats', 'purity'],\n",
      " ['errors', 'never', 'pass', 'silently'],\n",
      " ['explicitly', 'silenced'],\n",
      " ['face', 'ambiguity', 'refuse', 'temptation', 'guess'],\n",
      " ['one', 'preferably', 'one', 'obvious', 'way'],\n",
      " ['way', 'may', 'obvious', 'first', 'dutch'],\n",
      " ['better', 'never'],\n",
      " ['never', 'often', 'better', 'right'],\n",
      " ['implementation', 'hard', 'explain', 'bad', 'idea'],\n",
      " ['implementation', 'easy', 'explain', 'may', 'good', 'idea'],\n",
      " ['namespaces', 'one', 'honking', 'great', 'idea', 'let']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "tokens = tokenize_text(txt)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과\n",
    "```\n",
    "[['beautiful', 'better', 'ugly'],\n",
    " ['explicit', 'better', 'implicit'],\n",
    " ['simple', 'better', 'complex'],\n",
    " ['complex', 'better', 'complicated'],\n",
    " ['flat', 'better', 'nested'],\n",
    " ['sparse', 'better', 'dense'],\n",
    " ['readability', 'counts'],\n",
    " ['special', 'cases', 'special', 'enough', 'break', 'rules'],\n",
    " ['although', 'practicality', 'beats', 'purity'],\n",
    " ['errors', 'never', 'pass', 'silently'],\n",
    " ['unless', 'explicitly', 'silenced'],\n",
    " ['face', 'ambiguity', 'refuse', 'temptation', 'guess'],\n",
    " ['one', 'preferably', 'one', 'obvious', 'way'],\n",
    " ['although', 'way', 'may', 'obvious', 'first', 'unless', 'dutch'],\n",
    " ['better', 'never'],\n",
    " ['although', 'never', 'often', 'better', 'right'],\n",
    " ['implementation', 'hard', 'explain', 'bad', 'idea'],\n",
    " ['implementation', 'easy', 'explain', 'may', 'good', 'idea']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "- 형태소\n",
    "    - 일정한 의미가 있는 가장 작은 말의 단위\n",
    "- 형태소 분석  \n",
    "    - 말뭉치에서 의미있는(분석에 필요한) 형태소들만 추출하는 것\n",
    "    - 보통 단어로 부터 어근, 접두사, 접미사, 품사등 언어적 속성을 파악하여 처리한다. \n",
    "- 형태소 분석을 위한 기법\n",
    "    - 어간추출(Stemming)\n",
    "    - 원형(기본형) 복원 (Lemmatization)\n",
    "    - 품사부착 (POS tagging - Part Of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간추출(Stemming)\n",
    "- 어간: 활용어에서 변하지 않는 부분\n",
    "    - painted, paint, painting => 어간: paint\n",
    "    - 보다, 보니, 보고=>어간 보-\n",
    "- 어간 추출 \n",
    "    - 단어에서 어미를 제거하고 어간을 추출하는 작업\n",
    "- 목적\n",
    "    - 같은 의미를 가지는 단어의 여러가지 활용이 있을 경우 다른 단어로 카운트 되는 문제점을 해결한다.\n",
    "        - flower, flowers 가 두개의 단어로 카운트 되는 것을 flower로 통일한다.        \n",
    "- nltk의 주요 어간 추출 알고리즘\n",
    "    - Porter Stemmer\n",
    "    - Lancaster Stemmer\n",
    "    - Snowball Stemmer\n",
    "- 메소드\n",
    "    - `stemmer객체.stem(단어)`\n",
    "- stemming의 문제\n",
    "    - 완벽하지 않다는 것이 문제이다.        \n",
    "        - ex) new와 news는 다른 단어 인데 둘다 new로 처리한다.\n",
    "    - 처리후 눈으로 직접 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming은 단어들을 모두 소문자로 변환한 뒤 처리 하기 때문에 대소문자 섞여서도 상관없다. \n",
    "\n",
    "words = [\n",
    "    'Working', 'Works','Worked',\n",
    "    'Painting','Paints','Painted',\n",
    "    'happy','happier','happiest'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. stemmer 객체 생성 \n",
    "# 2. stem(단어)을 호출해서 단어별로 어간추출 작업을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps.stem('Working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'work', 'work', 'paint', 'paint', 'paint', 'happi', 'happier', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'work', 'work', 'paint', 'paint', 'paint', 'happy', 'happy', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "print([ls.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'work', 'work', 'paint', 'paint', 'paint', 'happi', 'happier', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "sbs = SnowballStemmer('english') # 스노우볼은 언어를 지정해줘야 한다. \n",
    "print([sbs.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원형(기본형)복원(Lemmatization)\n",
    "- 단어의 기본형을 반환한다.\n",
    "    - ex) am, is, are => be\n",
    "- 단어의 품사를 지정하면 정확한 결과를 얻을 수 있다. \n",
    "- `WordNetLemmatizer객체.lemmatize(단어 [, pos=품사])`\n",
    "\n",
    "> 어간추출과 원형복원은 문법적 또는 의미적으로 변한 단어의 원형을 찾는 역할은 동일하다.<br>\n",
    "> **원형복원**은 품사와 같은 문법적요소와 문장내에서의 의미적인 부분을 감안해 찾기 때문에 어간추출 방식보다 더 정교하다. \n",
    "\n",
    "\n",
    "- 소문자로 바꿔서 진행해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/master90/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['is', 'are', 'am', 'has','had','have']\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "lemm.lemmatize('are', pos = 'v') # pos : 품자를 지정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'be', 'be', 'have', 'have', 'have']\n"
     ]
    }
   ],
   "source": [
    "print([lemm.lemmatize(word, pos = 'v') for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'haapier', 'happy']\n"
     ]
    }
   ],
   "source": [
    "word2 = ['happy','haapier','happiest'] # 형용사는 a \n",
    "print([lemm.lemmatize(word, pos = 'a') for word in word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meet', 'meet')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbs.stem('meeting'), sbs.stem('meet') \n",
    "# meeting을 회의 명사로 쓰고 싶고, meet을 만나다 동사로 쓰고 싶은데 \n",
    "# 스노우로 하면 meet으로 나온다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meeting', 'meet')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize('meeting'), lemm.lemmatize('meet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meet', 'meeting')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize('meeting', pos= 'v'), lemm.lemmatize('meeting', pos='n')\n",
    "# 품사를 지정하면 그에 따라 출력된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사부착-POS Tagging(Part-Of-Speech Tagging)\n",
    "- 형태소에 품사를 붙이는 작업.\n",
    "    - 품사의 구분이나 표현은 언어, 학자마다 다르다. \n",
    "- NLTK는 펜 트리뱅크 태그세트(Penn Treebank Tagset) 이용\n",
    "    - 명사 : N으로 시작 (NN-일반명사, NNP-고유명사)\n",
    "    - 형용사 : J로 사직(JJ, JJR-비교급, JJS-최상급)\n",
    "    - 동사: V로 시작 (VB-동사, VBP-현재형 동사)\n",
    "    - 부사: R로 시작 (RB-부사)\n",
    "    - `nltk.help.upenn_tagset(['키워드'])` : 도움말\n",
    "- `pos_tag(단어_리스트)`    \n",
    "    - 단어와 품사를 튜플로 묶은 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 프로세스\n",
    "- 클렌징(cleansing)\n",
    "    - 특수문자, 기호 필요없는 문자 제거\n",
    "    - 대소문자 변경\n",
    "- stop word(분석에 필요 없는 토큰) 제거\n",
    "- 텍스트 토큰화\n",
    "    - 분석의 최소단위로 나누는 작업\n",
    "    - 보통 단어단위나 글자단위로 나눈다.\n",
    "- 어근 추출(Stemming/Lemmatization)을 통한 텍스트 정규화 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- tokenize_text에 stemming 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T22:13:03.286922Z",
     "start_time": "2020-09-07T22:13:03.266976Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize_text2(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과\n",
    "```\n",
    "[['beautiful', 'good', 'ugly'],\n",
    " ['explicit', 'well', 'implicit'],\n",
    " ['simple', 'well', 'complex'],\n",
    " ['complex', 'well', 'complicate'],\n",
    " ['flat', 'well', 'nest'],\n",
    " ['sparse', 'well', 'dense'],\n",
    " ['readability', 'count'],\n",
    " ['special', 'case', 'special', 'enough', 'break', 'rule'],\n",
    " ['practicality', 'beat', 'purity'],\n",
    " ['error', 'never', 'pass', 'silently'],\n",
    " ['explicitly', 'silence'],\n",
    " ['face', 'ambiguity', 'refuse', 'temptation', 'guess'],\n",
    " ['preferably', 'obvious', 'way'],\n",
    " ['way', 'obvious', 'first', 'dutch'],\n",
    " ['well', 'never'],\n",
    " ['never', 'often', 'good', 'right'],\n",
    " ['implementation', 'hard', 'explain', 'bad', 'idea'],\n",
    " ['implementation', 'easy', 'explain', 'good', 'idea']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## news.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분석을 위한 클래스들\n",
    "## Text클래스\n",
    "- 문서 분석에 유용한 여러 메소드 제공\n",
    "- **토큰 리스트**을 입력해 객체생성 후 제공되는 메소드를 이용해 분석한다.\n",
    "- ### 생성\n",
    "    - Text(토큰리스트, [name=이름])\n",
    "- ### 주요 메소드\n",
    "    - count(단어)\n",
    "        - 매개변수로 전달한 단어의 빈도수\n",
    "    - plot(N)\n",
    "        - 빈도수 상위 N개 단어를 선그래프로 시각화\n",
    "    - dispersion_plot(단어리스트)\n",
    "        - 매개변수로 전달한 단어들이 전체 말뭉치의 어느 부분에 나오는지 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqDist\n",
    "- document에서 사용된 토큰(단어)의 사용빈도 데이터를 가지는 클래스\n",
    "    - 토큰(단어)를 key, 개수를 value로 가지는 딕셔너리 형태\n",
    "- 생성\n",
    "    - Text 객체의 vocab() 메소드로 조회한다.\n",
    "    - 생성자(Initializer)에 토큰 List를 직접 넣어 생성가능\n",
    "- 주요 메소드\n",
    "    - B(): 출연한 고유 단어의 개수\n",
    "        - [Apple, Apple] -> 1\n",
    "    - N(): 총 단어수 \n",
    "        - [Apple, Apple] -> 2\n",
    "    - get(단어) 또는 FreqDist['단어'] : 특정 단어의 출연 빈도수\n",
    "    - freq(단어): 총 단어수 대비 특정단어의 출연비율\n",
    "    - most_common() : 빈도수 순서로 정렬하여 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn의 CountVectorizer를 이용해 TDM 만들기\n",
    "\n",
    "- ### TDM\n",
    "    - Term-Document-Matrix\n",
    "    - 문서안에서 문서를 구성하는 단어들이 몇번 나왔는지를 표현하는 행렬\n",
    "- TDM 행렬 반환\n",
    "    - 행: 문서\n",
    "    - 컬럼: 고유단어\n",
    "    - Value: 개수\n",
    "- 학습: fit(raw document)\n",
    "    - raw document - 문장을 원소로 가지는 1차원 배열형태 문서\n",
    "    - 전체 문장들(corpus)에서 고유단어들을 찾아낸뒤 index를 붙인다.\n",
    "- 변환: transform(raw document)\n",
    "    - 문장별(원소) 단어 count\n",
    "- CountVectorizer 주요 속성, 메소드\n",
    "    - cv.vocabulary_\n",
    "        - 단어-index 반환(딕셔너리)\n",
    "    - get_feature_names()\n",
    "        - index순서대로 단어들 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## news.txt\n",
    "- news.txt를 읽고 TDM 생성 후 csv파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
